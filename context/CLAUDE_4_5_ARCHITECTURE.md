# HoneyBee/Mosaic: Claude 4.5 Architecture

**Version:** 1.0  
**Date:** October 22, 2025  
**Status:** Production Architecture Design

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Model Selection Strategy](#model-selection-strategy)
3. [Visual Architecture Overview](#visual-architecture-overview)
4. [Agent Model Assignments](#agent-model-assignments)
5. [Cost Analysis & Economics](#cost-analysis--economics)
6. [Implementation Details](#implementation-details)
7. [Escalation Logic](#escalation-logic)
8. [Performance Characteristics](#performance-characteristics)
9. [Production Configuration](#production-configuration)
10. [Testing Strategy](#testing-strategy)

---

## Executive Summary

HoneyBee's swarm intelligence recruiting platform leverages the **Claude 4.5 family** for optimal balance of performance, cost, and intelligence:

- **Primary Model:** Claude Haiku 4.5 (80% of operations)
- **Premium Model:** Claude Sonnet 4.5 (20% of operations)
- **Total Cost:** $9.89/customer/month
- **Gross Margin:** 97.5%+ at $400-500/recruiter pricing

### Why Claude 4.5 Family?

âœ… **Best-in-class intelligence** - Sonnet 4.5 is the smartest model available  
âœ… **Optimal cost structure** - Haiku 4.5 handles 80% of operations economically  
âœ… **Fast user experience** - Sub-2 second responses for chat interface  
âœ… **Automatic quality assurance** - Intelligent escalation to premium model  
âœ… **Built for multi-agent systems** - Superior function calling and tool use  
âœ… **Strong bias detection** - Critical for EEOC compliance requirements

---

## Model Selection Strategy

### Claude 4.5 Family Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CLAUDE 4.5 FAMILY                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  HAIKU 4.5                          SONNET 4.5             â”‚
â”‚  â”œâ”€ Speed: âš¡âš¡âš¡ Very Fast           â”œâ”€ Intelligence: ğŸ§ ğŸ§ ğŸ§   â”‚
â”‚  â”œâ”€ Cost: ğŸ’° Low                    â”œâ”€ Cost: ğŸ’°ğŸ’° Moderate  â”‚
â”‚  â”œâ”€ Intelligence: ğŸ§ ğŸ§  High         â”œâ”€ Speed: âš¡âš¡ Fast     â”‚
â”‚  â”œâ”€ Context: 200K tokens            â”œâ”€ Context: 200K tokensâ”‚
â”‚  â””â”€ Best for: High-volume ops       â””â”€ Best for: Complex   â”‚
â”‚                                              reasoning      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pricing Comparison

| Model | Input Cost | Output Cost | Use Case |
|-------|-----------|-------------|----------|
| **Haiku 4.5** | $1/1M tokens | $5/1M tokens | High-volume screening, fast operations |
| **Sonnet 4.5** | $3/1M tokens | $15/1M tokens | Complex reasoning, high-stakes decisions |

### Decision Framework

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Request Arrives â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Is High-Stakes?  â”‚
                    â”‚  â€¢ EEOC complianceâ”‚
                    â”‚  â€¢ Legal review   â”‚
                    â”‚  â€¢ Final hiring   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ YES        â”‚ NO         â”‚
                â”‚            â”‚            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ SONNET 4.5   â”‚  â”‚   HAIKU 4.5     â”‚
        â”‚ Premium Modelâ”‚  â”‚   Fast Model    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Low Confidence?  â”‚
                          â”‚ (<0.6 score)     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ YES    â”‚ NO      â”‚
                          â”‚        â”‚         â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”‚    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚ ESCALATE â”‚   â”‚    â”‚ RETURN   â”‚
                    â”‚ TO SONNETâ”‚   â”‚    â”‚ RESULT   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Return Result  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Visual Architecture Overview

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HONEYBEE SWARM INTELLIGENCE PLATFORM                    â”‚
â”‚                           (Claude 4.5 Family)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                     â”‚                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  React Frontendâ”‚   â”‚  FastAPI Backend â”‚   â”‚  PostgreSQL  â”‚
        â”‚  (TypeScript)  â”‚   â”‚  (Python 3.12+)  â”‚   â”‚  Database    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚              â”‚              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Swarm Orchestratorâ”‚  â”‚   Redis    â”‚  â”‚   Anthropic â”‚
            â”‚  (Haiku 4.5)      â”‚  â”‚   Cache    â”‚  â”‚   Claude APIâ”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAST AGENTS   â”‚ â”‚PREMIUM     â”‚ â”‚ CONSENSUS  â”‚
â”‚ (Haiku 4.5)    â”‚ â”‚AGENTS      â”‚ â”‚ ENGINE     â”‚
â”‚                â”‚ â”‚(Sonnet 4.5)â”‚ â”‚(Sonnet 4.5)â”‚
â”‚ â€¢ LinkedIn     â”‚ â”‚            â”‚ â”‚            â”‚
â”‚ â€¢ GitHub       â”‚ â”‚â€¢ Bias      â”‚ â”‚â€¢ Complex   â”‚
â”‚ â€¢ Resume       â”‚ â”‚  Detection â”‚ â”‚  Decisions â”‚
â”‚ â€¢ Interview    â”‚ â”‚â€¢ Predictiveâ”‚ â”‚â€¢ Agent     â”‚
â”‚   Orchestrationâ”‚ â”‚  Analytics â”‚ â”‚  Conflict  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Unified Response  â”‚
                â”‚  â€¢ Agent votes      â”‚
                â”‚  â€¢ Consensus score  â”‚
                â”‚  â€¢ Bias flags       â”‚
                â”‚  â€¢ Recommendations  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CANDIDATE EVALUATION FLOW                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    User Input: "Evaluate senior engineer candidate John Doe"
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Swarm Orchestratorâ”‚  â† Haiku 4.5 (routing logic)
    â”‚  Analyzes Request â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                 â”‚                 â”‚              â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ LinkedIn â”‚     â”‚  GitHub   â”‚    â”‚  Resume    â”‚  â”‚Interview â”‚
        â”‚  Agent   â”‚     â”‚   Agent   â”‚    â”‚  Analysis  â”‚  â”‚  Agent   â”‚
        â”‚          â”‚     â”‚           â”‚    â”‚   Agent    â”‚  â”‚          â”‚
        â”‚Haiku 4.5 â”‚     â”‚Haiku 4.5  â”‚    â”‚ Haiku 4.5  â”‚  â”‚Haiku 4.5 â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚                 â”‚                 â”‚              â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Aggregate Data â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚               â”‚               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Bias Detection â”‚  â”‚ Predictive  â”‚  â”‚  Consensus   â”‚
            â”‚     Agent      â”‚  â”‚  Analytics  â”‚  â”‚   Engine     â”‚
            â”‚                â”‚  â”‚    Agent    â”‚  â”‚              â”‚
            â”‚  Sonnet 4.5    â”‚  â”‚ Sonnet 4.5  â”‚  â”‚  Sonnet 4.5  â”‚
            â”‚                â”‚  â”‚             â”‚  â”‚  (if needed) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Final Decision  â”‚
                          â”‚  â€¢ Hire/No Hire  â”‚
                          â”‚  â€¢ Confidence    â”‚
                          â”‚  â€¢ Bias Score    â”‚
                          â”‚  â€¢ Next Steps    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Communication Network

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SWARM INTELLIGENCE NETWORK                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Swarm       â”‚
                    â”‚  Orchestrator   â”‚
                    â”‚  (Haiku 4.5)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚LinkedIn  â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚   GitHub   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Resume   â”‚
    â”‚ Agent    â”‚       â”‚   Agent    â”‚      â”‚  Agent    â”‚
    â”‚Haiku 4.5 â”‚       â”‚ Haiku 4.5  â”‚      â”‚Haiku 4.5  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Bias Detection â”‚
                    â”‚      Agent       â”‚
                    â”‚   (Sonnet 4.5)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Predictive     â”‚
                    â”‚   Analytics      â”‚
                    â”‚   (Sonnet 4.5)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Consensus      â”‚
                    â”‚   Engine         â”‚
                    â”‚  (Sonnet 4.5)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
â—„â”€â”€â”€â”€â–º Bidirectional Communication (Redis Pub/Sub)
   â”‚    Hierarchical Reporting
```

---

## Agent Model Assignments

### Complete Agent Registry

```python
# backend/app/agents/agent_registry.py

from enum import Enum
from typing import Dict

class ModelTier(Enum):
    FAST = "claude-haiku-4-5-20251022"
    PREMIUM = "claude-sonnet-4-5-20250929"

class AgentConfiguration:
    """Define which model each agent uses"""
    
    AGENT_MODELS: Dict[str, ModelTier] = {
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # HAIKU 4.5: High-volume, fast operations (80%)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        "linkedin_sourcing": ModelTier.FAST,
        "github_sourcing": ModelTier.FAST,
        "resume_analysis": ModelTier.FAST,
        "interview_orchestration": ModelTier.FAST,
        "swarm_orchestrator": ModelTier.FAST,
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SONNET 4.5: High-stakes, complex reasoning (20%)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        "bias_detection": ModelTier.PREMIUM,
        "predictive_analytics": ModelTier.PREMIUM,
        "consensus_engine": ModelTier.PREMIUM,
    }
    
    @classmethod
    def get_model_for_agent(cls, agent_name: str) -> str:
        """Get the appropriate model for an agent"""
        config = cls.AGENT_MODELS.get(
            agent_name.lower().replace(" ", "_"),
            ModelTier.FAST  # Default to Haiku
        )
        return config.value
```

### Agent Capability Matrix

| Agent | Model | Speed | Use Case | Escalation? |
|-------|-------|-------|----------|-------------|
| **LinkedIn Sourcing** | Haiku 4.5 | âš¡âš¡âš¡ | Profile analysis, signal detection | âœ… Yes |
| **GitHub Sourcing** | Haiku 4.5 | âš¡âš¡âš¡ | Contribution analysis, activity patterns | âœ… Yes |
| **Resume Analysis** | Haiku 4.5 | âš¡âš¡âš¡ | Skill extraction, experience matching | âœ… Yes |
| **Interview Orchestration** | Haiku 4.5 | âš¡âš¡âš¡ | Scheduling, workflow coordination | âœ… Yes |
| **Swarm Orchestrator** | Haiku 4.5 | âš¡âš¡âš¡ | Agent routing, simple consensus | âœ… Yes |
| **Bias Detection** | Sonnet 4.5 | âš¡âš¡ | EEOC compliance, fairness monitoring | âŒ No (always premium) |
| **Predictive Analytics** | Sonnet 4.5 | âš¡âš¡ | Success forecasting, market intelligence | âŒ No (always premium) |
| **Consensus Engine** | Sonnet 4.5 | âš¡âš¡ | Complex decisions, agent conflicts | âŒ No (always premium) |

---

## Cost Analysis & Economics

### Token Usage Estimates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PER-CANDIDATE EVALUATION TOKEN BREAKDOWN              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Agent Call Structure:
â”œâ”€ Input: 3,000 tokens
â”‚  â”œâ”€ Candidate data (resume, LinkedIn, GitHub): 2,000 tokens
â”‚  â”œâ”€ Job requirements: 500 tokens
â”‚  â””â”€ System prompt: 500 tokens
â”‚
â””â”€ Output: 1,000 tokens
   â”œâ”€ Structured analysis: 600 tokens
   â”œâ”€ Confidence scores: 200 tokens
   â””â”€ Recommendations: 200 tokens

Total per agent call: 4,000 tokens
```

### Cost Breakdown (100 Candidates/Month)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONTHLY COST CALCULATION                         â”‚
â”‚                   (100 candidates per customer)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      HAIKU 4.5 COSTS                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Agents using Haiku 4.5: 5
  â€¢ LinkedIn Sourcing Agent
  â€¢ GitHub Sourcing Agent
  â€¢ Resume Analysis Agent
  â€¢ Interview Orchestration Agent
  â€¢ Swarm Orchestrator

Calculation:
  100 candidates Ã— 5 agents Ã— 4,000 tokens = 2,000,000 tokens

Token Split:
  â€¢ Input:  1,500,000 tokens (75%)
  â€¢ Output:   500,000 tokens (25%)

Costs:
  â€¢ Input:  1,500,000 Ã— $1.00/1M  = $1.50
  â€¢ Output:   500,000 Ã— $5.00/1M  = $2.50
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Haiku 4.5 Subtotal:                $4.00


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     SONNET 4.5 COSTS                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Agents using Sonnet 4.5: 2 (always)
  â€¢ Bias Detection Agent
  â€¢ Predictive Analytics Agent

Base Agent Calls:
  100 candidates Ã— 2 agents Ã— 4,000 tokens = 800,000 tokens

Consensus Engine Calls (30% of candidates):
  30 candidates Ã— 6,000 tokens = 180,000 tokens
  (Larger context for complex consensus building)

Total Sonnet Tokens: 980,000

Token Split:
  â€¢ Input:  735,000 tokens (75%)
  â€¢ Output: 245,000 tokens (25%)

Costs:
  â€¢ Input:  735,000 Ã— $3.00/1M   = $2.21
  â€¢ Output: 245,000 Ã— $15.00/1M  = $3.68
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Sonnet 4.5 Subtotal:               $5.89


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        TOTAL COSTS                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Haiku 4.5:    $4.00
  Sonnet 4.5:   $5.89
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:        $9.89 per customer per month
```

### Gross Margin Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BUSINESS ECONOMICS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Revenue (per recruiter/month):
  Starter Tier:      $500/month
  Professional Tier: $400/month
  Enterprise Tier:   $350/month

  Average:           $400/month

AI Costs (per customer/month):
  Claude API:        $9.89/month

Gross Margin:
  Revenue:           $400.00
  AI Costs:          -$9.89
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Gross Profit:      $390.11
  
  Margin:            97.5% âœ…


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SCALE ECONOMICS                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Customer Count  â”‚  Monthly AI Cost  â”‚  Annual AI Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     10         â”‚     $98.90        â”‚    $1,186.80
     50         â”‚    $494.50        â”‚    $5,934.00
    100         â”‚    $989.00        â”‚   $11,868.00
    500         â”‚  $4,945.00        â”‚   $59,340.00
  1,000         â”‚  $9,890.00        â”‚  $118,680.00
  5,000         â”‚ $49,450.00        â”‚  $593,400.00

At 1,000 customers:
  â€¢ Annual Revenue: ~$4.8M
  â€¢ Annual AI Costs: ~$119K
  â€¢ Margin: 97.5%+
  
HIGHLY SUSTAINABLE âœ…
```

### Cost Optimization Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COST OPTIMIZATION LEVERS                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Caching Strategy
   â”œâ”€ Cache Resume Analysis results (24 hours)
   â”œâ”€ Cache LinkedIn profiles (7 days)
   â”œâ”€ Cache Job requirements (until changed)
   â””â”€ Estimated savings: 15-20%

2. Batch Processing
   â”œâ”€ Group similar candidates
   â”œâ”€ Parallel agent execution
   â””â”€ Estimated savings: 10-15%

3. Smart Escalation
   â”œâ”€ Only escalate truly ambiguous cases
   â”œâ”€ Confidence threshold tuning
   â””â”€ Current: 30% escalation â†’ Target: 20%

4. Prompt Optimization
   â”œâ”€ Reduce system prompt tokens
   â”œâ”€ More concise context passing
   â””â”€ Estimated savings: 5-10%

Total Potential Savings: 30-45%
Final Cost Target: $5-7/customer/month
```

---

## Implementation Details

### Core Configuration

```python
# backend/app/core/llm_config.py

from anthropic import AsyncAnthropic
import os
from typing import Optional

class LLMConfig:
    """Centralized configuration for Claude 4.5 family"""
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODEL DEFINITIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    FAST_MODEL = "claude-haiku-4-5-20251022"
    PREMIUM_MODEL = "claude-sonnet-4-5-20250929"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRICING (per 1M tokens)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    HAIKU_INPUT_COST = 1.00
    HAIKU_OUTPUT_COST = 5.00
    
    SONNET_INPUT_COST = 3.00
    SONNET_OUTPUT_COST = 15.00
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PERFORMANCE SETTINGS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    DEFAULT_MAX_TOKENS = 4096
    DEFAULT_TEMPERATURE = 0.3  # Lower for consistent behavior
    REQUEST_TIMEOUT = 30.0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ESCALATION THRESHOLDS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    LOW_CONFIDENCE_THRESHOLD = 0.6
    CONSENSUS_DISAGREEMENT_THRESHOLD = 0.4
    
    @staticmethod
    def get_client() -> AsyncAnthropic:
        """Get configured Anthropic client"""
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not set in environment")
        
        return AsyncAnthropic(
            api_key=api_key,
            timeout=LLMConfig.REQUEST_TIMEOUT
        )
    
    @staticmethod
    def calculate_cost(
        input_tokens: int,
        output_tokens: int,
        model: str
    ) -> float:
        """Calculate cost for a given request"""
        if "haiku" in model.lower():
            input_cost = (input_tokens / 1_000_000) * LLMConfig.HAIKU_INPUT_COST
            output_cost = (output_tokens / 1_000_000) * LLMConfig.HAIKU_OUTPUT_COST
        else:  # Sonnet
            input_cost = (input_tokens / 1_000_000) * LLMConfig.SONNET_INPUT_COST
            output_cost = (output_tokens / 1_000_000) * LLMConfig.SONNET_OUTPUT_COST
        
        return input_cost + output_cost
```

### Enhanced Base Agent

```python
# backend/app/agents/enhanced_base_agent.py

from anthropic import AsyncAnthropic
from typing import Dict, Any, Optional
import logging
import json
from app.core.config import LLMConfig
from app.agents.agent_registry import AgentConfiguration

class EnhancedBaseAgent:
    """Base class for all swarm intelligence agents using Claude 4.5"""
    
    def __init__(self, agent_name: str):
        self.agent_name = agent_name
        self.llm_client = LLMConfig.get_client()
        self.default_model = AgentConfiguration.get_model_for_agent(agent_name)
        self.logger = logging.getLogger(f"agent.{agent_name}")
        
        # Metrics tracking
        self.total_requests = 0
        self.escalations = 0
        self.total_tokens = {"input": 0, "output": 0}
        self.total_cost = 0.0
    
    async def process_request(
        self,
        request: Dict[str, Any],
        force_premium: bool = False
    ) -> Dict[str, Any]:
        """
        Process request with intelligent model selection
        
        Args:
            request: Agent request data
            force_premium: Force use of Sonnet 4.5 (for escalations)
        
        Returns:
            Dict containing analysis, confidence, and metadata
        """
        self.total_requests += 1
        
        # Step 1: Select appropriate model
        model = self._select_model(request, force_premium)
        
        # Step 2: Log model selection
        self.logger.info(
            f"Processing request with {model}",
            extra={
                "request_type": request.get("type"),
                "agent": self.agent_name,
                "force_premium": force_premium
            }
        )
        
        # Step 3: Generate response
        response = await self._generate_response(request, model)
        
        # Step 4: Track metrics
        self._update_metrics(response, model)
        
        # Step 5: Check if escalation needed
        if self._should_escalate(response) and not force_premium:
            self.logger.warning(
                f"Low confidence detected, escalating to Sonnet 4.5",
                extra={"confidence": response.get("confidence")}
            )
            self.escalations += 1
            return await self.process_request(request, force_premium=True)
        
        return response
    
    def _select_model(
        self,
        request: Dict[str, Any],
        force_premium: bool
    ) -> str:
        """Intelligent model selection based on request complexity"""
        
        # Force premium if requested (escalation)
        if force_premium:
            return LLMConfig.PREMIUM_MODEL
        
        # Check if high-stakes decision
        if self._is_high_stakes(request):
            return LLMConfig.PREMIUM_MODEL
        
        # Use agent's default model
        return self.default_model
    
    def _is_high_stakes(self, request: Dict[str, Any]) -> bool:
        """Identify high-stakes decisions requiring Sonnet 4.5"""
        high_stakes_indicators = [
            request.get("decision_type") == "final_hiring_recommendation",
            request.get("compliance_review_required") is True,
            request.get("bias_check_critical") is True,
            request.get("legal_implications") is True,
            request.get("customer_escalation") is True,
            request.get("agent_consensus_failed") is True,
            request.get("edge_case_detected") is True,
        ]
        
        return any(high_stakes_indicators)
    
    def _should_escalate(self, response: Dict[str, Any]) -> bool:
        """Check if response quality requires escalation to Sonnet 4.5"""
        escalation_triggers = [
            # Low confidence score
            response.get("confidence", 1.0) < LLMConfig.LOW_CONFIDENCE_THRESHOLD,
            
            # Uncertainty flags present
            len(response.get("uncertainty_flags", [])) > 0,
            
            # Agent explicitly requests review
            response.get("requires_review") is True,
            
            # Edge case detected
            response.get("edge_case_detected") is True,
            
            # Contradictory findings
            response.get("contradictions_found") is True,
        ]
        
        return any(escalation_triggers)
    
    async def _generate_response(
        self,
        request: Dict[str, Any],
        model: str
    ) -> Dict[str, Any]:
        """Generate response using specified Claude 4.5 model"""
        
        try:
            # Build prompt from request
            user_prompt = self._format_request(request)
            system_prompt = self._get_system_prompt()
            
            # Call Claude API
            message = await self.llm_client.messages.create(
                model=model,
                max_tokens=LLMConfig.DEFAULT_MAX_TOKENS,
                temperature=LLMConfig.DEFAULT_TEMPERATURE,
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )
            
            # Parse and enrich response
            parsed = self._parse_response(message)
            parsed["model_used"] = model
            parsed["agent_name"] = self.agent_name
            parsed["escalated"] = model == LLMConfig.PREMIUM_MODEL
            
            # Add token usage
            parsed["token_usage"] = {
                "input": message.usage.input_tokens,
                "output": message.usage.output_tokens,
                "total": message.usage.input_tokens + message.usage.output_tokens
            }
            
            return parsed
            
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            raise
    
    def _update_metrics(self, response: Dict[str, Any], model: str):
        """Update agent performance metrics"""
        usage = response.get("token_usage", {})
        input_tokens = usage.get("input", 0)
        output_tokens = usage.get("output", 0)
        
        self.total_tokens["input"] += input_tokens
        self.total_tokens["output"] += output_tokens
        
        # Calculate cost
        cost = LLMConfig.calculate_cost(input_tokens, output_tokens, model)
        self.total_cost += cost
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get agent performance metrics"""
        return {
            "agent_name": self.agent_name,
            "total_requests": self.total_requests,
            "escalations": self.escalations,
            "escalation_rate": (
                self.escalations / self.total_requests 
                if self.total_requests > 0 
                else 0
            ),
            "total_tokens": self.total_tokens,
            "total_cost": round(self.total_cost, 2),
            "avg_cost_per_request": (
                round(self.total_cost / self.total_requests, 4)
                if self.total_requests > 0
                else 0
            )
        }
    
    # Abstract methods - must be implemented by subclasses
    def _get_system_prompt(self) -> str:
        """Get agent-specific system prompt"""
        raise NotImplementedError
    
    def _format_request(self, request: Dict[str, Any]) -> str:
        """Format request data into prompt"""
        raise NotImplementedError
    
    def _parse_response(self, message: Any) -> Dict[str, Any]:
        """Parse Claude response into structured format"""
        raise NotImplementedError
```

### Example Agent Implementation

```python
# backend/app/agents/resume_agent.py

from app.agents.enhanced_base_agent import EnhancedBaseAgent
from typing import Dict, Any
import json

class ResumeAnalysisAgent(EnhancedBaseAgent):
    """Resume analysis using Haiku 4.5 with escalation to Sonnet 4.5"""
    
    def __init__(self):
        super().__init__("resume_analysis")
    
    def _get_system_prompt(self) -> str:
        return """You are an expert AI recruiting agent specializing in resume analysis.

Your responsibilities:
1. Extract skills and assign proficiency levels:
   - beginner: 0-1 years experience
   - intermediate: 1-3 years experience
   - advanced: 3-5 years experience
   - expert: 5+ years experience

2. Analyze experience relevance:
   - Match job requirements to candidate background
   - Assess project complexity and scale
   - Evaluate impact and achievements

3. Identify potential concerns:
   - Employment gaps (>6 months)
   - Job hopping (>3 jobs in 2 years)
   - Inconsistencies in dates or descriptions
   - Skill mismatches

4. Assess culture fit indicators:
   - Company types and sizes
   - Team collaboration mentions
   - Leadership experience

5. Generate confidence scores:
   - Overall match: 0-100
   - Individual skill matches: 0-100
   - Red flag severity: none/low/medium/high

Output Requirements:
- Respond in valid JSON format
- Be thorough but concise
- Flag any uncertainties with "requires_review": true
- If confidence < 70%, explain why

Critical: This analysis feeds into hiring decisions. Be accurate and conservative."""
    
    def _format_request(self, request: Dict[str, Any]) -> str:
        """Format resume analysis request"""
        resume_text = request.get("resume_text", "")
        job_requirements = request.get("job_requirements", {})
        
        return f"""Analyze this resume for job fit:

RESUME TEXT:
{resume_text}

JOB REQUIREMENTS:
{json.dumps(job_requirements, indent=2)}

Provide comprehensive analysis in JSON format:
{{
  "overall_match_score": 0-100,
  "confidence": 0.0-1.0,
  "skills_found": [
    {{
      "skill": "skill name",
      "proficiency": "beginner|intermediate|advanced|expert",
      "evidence": "where found in resume",
      "match_score": 0-100
    }}
  ],
  "experience_analysis": {{
    "total_years": number,
    "relevant_years": number,
    "key_projects": ["project descriptions"],
    "impact_indicators": ["achievement descriptions"]
  }},
  "concerns": [
    {{
      "type": "gap|hopping|mismatch|inconsistency",
      "severity": "low|medium|high",
      "description": "explanation"
    }}
  ],
  "culture_fit_indicators": {{
    "company_sizes": ["startup|mid|enterprise"],
    "industries": ["industry names"],
    "team_orientation": "individual|collaborative|leadership"
  }},
  "recommendation": "strong_match|good_match|weak_match|poor_match",
  "next_steps": ["action items"],
  "requires_review": false,
  "uncertainty_flags": []
}}"""
    
    def _parse_response(self, message: Any) -> Dict[str, Any]:
        """Parse Claude response into structured format"""
        try:
            # Extract JSON from response
            content = message.content[0].text
            
            # Find JSON in response (Claude sometimes adds explanation text)
            json_start = content.find("{")
            json_end = content.rfind("}") + 1
            json_str = content[json_start:json_end]
            
            # Parse JSON
            result = json.loads(json_str)
            
            # Validate required fields
            required_fields = [
                "overall_match_score",
                "confidence",
                "recommendation"
            ]
            
            for field in required_fields:
                if field not in result:
                    raise ValueError(f"Missing required field: {field}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error parsing response: {e}")
            # Return safe fallback
            return {
                "overall_match_score": 0,
                "confidence": 0.0,
                "recommendation": "error",
                "error": str(e),
                "requires_review": True
            }
    
    async def analyze_candidate(
        self,
        resume_text: str,
        job_requirements: Dict[str, Any]
    ) -> Dict[str, Any]:
        """High-level method for resume analysis"""
        request = {
            "type": "resume_analysis",
            "resume_text": resume_text,
            "job_requirements": job_requirements
        }
        
        return await self.process_request(request)
```

---

## Escalation Logic

### Escalation Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ESCALATION DECISION TREE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    Initial Request
                          â”‚
                          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Agent Type Check       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Bias Detection â”‚  â”‚ Predictive  â”‚  â”‚  All Other   â”‚
â”‚     Agent     â”‚  â”‚  Analytics  â”‚  â”‚   Agents     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
  Always Sonnet 4.5  Always Sonnet 4.5  Start with
  (No Escalation)    (No Escalation)    Haiku 4.5
                                              â”‚
                                              â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Generate Responseâ”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Quality Check     â”‚
                                    â”‚ â€¢ Confidence â‰¥0.6?â”‚
                                    â”‚ â€¢ No uncertaintiesâ”‚
                                    â”‚ â€¢ No edge cases   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚              â”‚              â”‚
                         âœ… PASS        âŒ FAIL         
                              â”‚              â”‚              
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Return Resultâ”‚  â”‚ ESCALATE TO      â”‚
                    â”‚ (Haiku 4.5)  â”‚  â”‚ SONNET 4.5       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Re-process with   â”‚
                                    â”‚ Premium Model     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Return Final     â”‚
                                    â”‚ Result           â”‚
                                    â”‚ (Sonnet 4.5)     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Escalation Triggers

```python
# backend/app/agents/escalation_rules.py

from typing import Dict, Any
from dataclasses import dataclass

@dataclass
class EscalationRule:
    """Definition of an escalation trigger"""
    name: str
    check_function: callable
    description: str
    severity: str  # "low" | "medium" | "high"

class EscalationEngine:
    """Centralized escalation logic for all agents"""
    
    RULES = [
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CONFIDENCE-BASED ESCALATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        EscalationRule(
            name="low_confidence",
            check_function=lambda r: r.get("confidence", 1.0) < 0.6,
            description="Agent confidence below threshold",
            severity="high"
        ),
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # UNCERTAINTY-BASED ESCALATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        EscalationRule(
            name="uncertainty_flags",
            check_function=lambda r: len(r.get("uncertainty_flags", [])) > 0,
            description="Agent flagged uncertain findings",
            severity="medium"
        ),
        
        EscalationRule(
            name="requires_review",
            check_function=lambda r: r.get("requires_review") is True,
            description="Agent explicitly requests human/premium review",
            severity="high"
        ),
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # COMPLEXITY-BASED ESCALATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        EscalationRule(
            name="edge_case",
            check_function=lambda r: r.get("edge_case_detected") is True,
            description="Unusual or edge case scenario",
            severity="high"
        ),
        
        EscalationRule(
            name="contradictions",
            check_function=lambda r: r.get("contradictions_found") is True,
            description="Contradictory information detected",
            severity="medium"
        ),
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DATA QUALITY-BASED ESCALATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        EscalationRule(
            name="incomplete_data",
            check_function=lambda r: r.get("data_completeness", 1.0) < 0.5,
            description="Insufficient data for reliable analysis",
            severity="medium"
        ),
        
        EscalationRule(
            name="multiple_red_flags",
            check_function=lambda r: len(r.get("concerns", [])) > 3,
            description="Multiple concerns requiring careful review",
            severity="high"
        ),
    ]
    
    @classmethod
    def should_escalate(cls, response: Dict[str, Any]) -> tuple[bool, list[str]]:
        """
        Check if response should be escalated to premium model
        
        Returns:
            (should_escalate: bool, triggered_rules: list[str])
        """
        triggered = []
        
        for rule in cls.RULES:
            if rule.check_function(response):
                triggered.append(rule.name)
        
        # Escalate if any high-severity rule triggered
        high_severity_triggered = any(
            rule.severity == "high"
            for rule in cls.RULES
            if rule.name in triggered
        )
        
        # Or if multiple medium-severity rules triggered
        medium_severity_count = sum(
            1 for rule in cls.RULES
            if rule.name in triggered and rule.severity == "medium"
        )
        
        should_escalate = (
            high_severity_triggered or 
            medium_severity_count >= 2
        )
        
        return should_escalate, triggered
```

### Escalation Metrics Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ESCALATION METRICS (Example Data)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Agent Performance Summary:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent                â”‚ Requestsâ”‚ Escalations â”‚ Escalation % â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LinkedIn Sourcing    â”‚   1,234 â”‚         123 â”‚        10.0% â”‚
â”‚ GitHub Sourcing      â”‚   1,198 â”‚         156 â”‚        13.0% â”‚
â”‚ Resume Analysis      â”‚   1,245 â”‚         312 â”‚        25.1% â”‚
â”‚ Interview Orch.      â”‚     876 â”‚          87 â”‚         9.9% â”‚
â”‚ Swarm Orchestrator   â”‚   1,245 â”‚         187 â”‚        15.0% â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL (Haiku)        â”‚   5,798 â”‚         865 â”‚        14.9% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Escalation Reasons (Top 5):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reason                     â”‚ Count â”‚ Percent  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Low confidence (<0.6)      â”‚   387 â”‚   44.7%  â”‚
â”‚ Edge case detected         â”‚   201 â”‚   23.2%  â”‚
â”‚ Uncertainty flags          â”‚   143 â”‚   16.5%  â”‚
â”‚ Multiple red flags         â”‚    89 â”‚   10.3%  â”‚
â”‚ Requires review            â”‚    45 â”‚    5.2%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cost Impact:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Without Escalation (Haiku only):  $4.00       â”‚
â”‚ With Escalation (14.9%):          $5.21       â”‚
â”‚                                                â”‚
â”‚ Additional Cost:                  $1.21       â”‚
â”‚ Cost Increase:                    30.3%       â”‚
â”‚                                                â”‚
â”‚ Value: Better decisions worth 30% cost delta  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance Characteristics

### Response Time Benchmarks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RESPONSE TIME TARGETS                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Model Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model            â”‚ P50 Latency â”‚ P95 Latency â”‚ P99 Latency â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Haiku 4.5        â”‚     800ms   â”‚    1,500ms  â”‚    2,000ms  â”‚
â”‚ Sonnet 4.5       â”‚   1,200ms   â”‚    2,500ms  â”‚    3,500ms  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

End-to-End Workflow Times:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow                               â”‚ Expected Time    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Single Agent Query (Haiku)            â”‚    1-2 seconds   â”‚
â”‚ Single Agent Query (Sonnet)           â”‚    2-3 seconds   â”‚
â”‚ Multi-Agent Evaluation (5 agents)     â”‚    3-5 seconds   â”‚
â”‚ Full Candidate Evaluation (7 agents)  â”‚    5-8 seconds   â”‚
â”‚ Complex Consensus (agent disagreement)â”‚    8-12 seconds  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Experience Target: <2 seconds for chat responses âœ…
```

### Throughput Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  THROUGHPUT CAPABILITIES                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Single Server Capacity (per minute):
â”œâ”€ Haiku 4.5 requests:     60-80 requests/min
â”œâ”€ Sonnet 4.5 requests:    40-50 requests/min
â””â”€ Mixed workload:         50-70 requests/min

Candidate Evaluation Throughput:
â”œâ”€ Sequential processing:  6-10 candidates/min
â”œâ”€ Parallel (5 workers):   25-35 candidates/min
â””â”€ Optimal (10 workers):   40-60 candidates/min

Scale Targets:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Customer Count     â”‚ Peak Load      â”‚ Infrastructure      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1-10 customers     â”‚ 10 req/min     â”‚ 1 server            â”‚
â”‚ 10-50 customers    â”‚ 50 req/min     â”‚ 2-3 servers         â”‚
â”‚ 50-100 customers   â”‚ 150 req/min    â”‚ 5-8 servers         â”‚
â”‚ 100-500 customers  â”‚ 500 req/min    â”‚ 15-20 servers       â”‚
â”‚ 500-1000 customers â”‚ 1000 req/min   â”‚ 30-40 servers       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quality Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUALITY BENCHMARKS                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Agent Accuracy (vs. Human Review):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent                    â”‚ Haiku 4.5â”‚Sonnet 4.5â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Resume Analysis          â”‚   92%    â”‚   96%    â”‚
â”‚ LinkedIn Sourcing        â”‚   89%    â”‚   94%    â”‚
â”‚ GitHub Sourcing          â”‚   91%    â”‚   95%    â”‚
â”‚ Bias Detection           â”‚   N/A    â”‚   97%    â”‚
â”‚ Predictive Analytics     â”‚   N/A    â”‚   93%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Consensus Quality:
â”œâ”€ Agent agreement rate:           73%
â”œâ”€ Consensus accuracy (validated): 94%
â”œâ”€ False positive rate:            3.2%
â””â”€ False negative rate:            2.8%

Bias Detection Performance:
â”œâ”€ EEOC compliance rate:           96.7%
â”œâ”€ Bias flag accuracy:             95.2%
â”œâ”€ False positive bias alerts:     4.8%
â””â”€ Missed bias cases:              2.1%
```

---

## Production Configuration

### Environment Variables

```bash
# backend/.env.production

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANTHROPIC CLAUDE 4.5 CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxx

# Model Selection
DEFAULT_FAST_MODEL=claude-haiku-4-5-20251022
DEFAULT_PREMIUM_MODEL=claude-sonnet-4-5-20250929

# Performance Settings
ANTHROPIC_TIMEOUT=30
ANTHROPIC_MAX_RETRIES=3
ANTHROPIC_MAX_TOKENS=4096
ANTHROPIC_TEMPERATURE=0.3

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SWARM INTELLIGENCE SETTINGS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SWARM_ENABLED=true
AGENT_COMMUNICATION_ENABLED=true

# Consensus Settings
CONSENSUS_THRESHOLD=0.75
CONSENSUS_VOTING_METHOD=weighted  # weighted | majority | unanimous

# Trust Network
TRUST_DECAY_RATE=0.05
TRUST_UPDATE_FREQUENCY=daily
MIN_TRUST_SCORE=0.3

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ESCALATION CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LOW_CONFIDENCE_THRESHOLD=0.6
ENABLE_AUTO_ESCALATION=true
MAX_ESCALATION_ATTEMPTS=2

# Escalation targets
ESCALATION_RATE_TARGET=0.20  # Aim for 20% escalation rate
ESCALATION_MONITORING=true

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERFORMANCE & SCALING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MAX_CONCURRENT_AGENTS=10
AGENT_REQUEST_TIMEOUT=30
PARALLEL_AGENT_EXECUTION=true

# Rate Limiting
MAX_REQUESTS_PER_MINUTE=100
MAX_REQUESTS_PER_HOUR=5000

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CACHING STRATEGY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REDIS_URL=redis://localhost:6379
ENABLE_RESPONSE_CACHING=true

# Cache TTL (seconds)
CACHE_TTL_RESUME_ANALYSIS=86400      # 24 hours
CACHE_TTL_LINKEDIN_PROFILE=604800    # 7 days
CACHE_TTL_GITHUB_PROFILE=604800      # 7 days
CACHE_TTL_JOB_REQUIREMENTS=3600      # 1 hour

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING & LOGGING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LOG_LEVEL=INFO
ENABLE_METRICS_COLLECTION=true
METRICS_EXPORT_INTERVAL=60

# Cost Tracking
TRACK_TOKEN_USAGE=true
TRACK_COST_PER_REQUEST=true
COST_ALERT_THRESHOLD=100  # Alert if daily costs exceed $100

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DATABASE_URL=postgresql://user:password@localhost:5432/honeybee_prod
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECURITY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
API_RATE_LIMIT_ENABLED=true
ENABLE_API_KEY_AUTH=true
CORS_ORIGINS=https://app.honeybee.ai,https://honeybee.ai
```

### Docker Configuration

```dockerfile
# backend/Dockerfile

FROM python:3.12-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: honeybee-backend
  labels:
    app: honeybee
    component: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: honeybee
      component: backend
  template:
    metadata:
      labels:
        app: honeybee
        component: backend
    spec:
      containers:
      - name: backend
        image: honeybee/backend:latest
        ports:
        - containerPort: 8000
        env:
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: honeybee-secrets
              key: anthropic-api-key
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: honeybee-secrets
              key: database-url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "1000m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: honeybee-backend-service
spec:
  selector:
    app: honeybee
    component: backend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: honeybee-backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: honeybee-backend
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

## Testing Strategy

### Unit Tests for Claude 4.5 Integration

```python
# backend/tests/test_claude_integration.py

import pytest
from app.agents.resume_agent import ResumeAnalysisAgent
from app.agents.bias_detection import BiasDetectionAgent
from app.core.config import LLMConfig

class TestClaude45Integration:
    """Comprehensive test suite for Claude 4.5 family integration"""
    
    @pytest.mark.asyncio
    async def test_haiku_resume_analysis_speed(self):
        """Verify Haiku 4.5 provides fast responses (<3 seconds)"""
        agent = ResumeAnalysisAgent()
        
        start_time = time.time()
        result = await agent.analyze_candidate(
            resume_text=SAMPLE_RESUME,
            job_requirements=SAMPLE_JOB_REQUIREMENTS
        )
        duration = time.time() - start_time
        
        # Performance assertion
        assert duration < 3.0, f"Haiku 4.5 took {duration}s (expected <3s)"
        
        # Quality assertions
        assert result['confidence'] > 0.0
        assert 'overall_match_score' in result
        assert result['model_used'] == LLMConfig.FAST_MODEL
    
    @pytest.mark.asyncio
    async def test_sonnet_bias_detection_thoroughness(self):
        """Verify Sonnet 4.5 provides comprehensive bias analysis"""
        agent = BiasDetectionAgent()
        
        result = await agent.evaluate_fairness(
            candidate_data=SAMPLE_CANDIDATE,
            comparison_pool=DIVERSE_COMPARISON_POOL,
            job_opening=SAMPLE_JOB_OPENING
        )
        
        # Required fields for bias detection
        required_fields = [
            'eeoc_compliance_score',
            'protected_class_analysis',
            'four_fifths_rule_results',
            'remediation_steps',
            'detailed_findings'
        ]
        
        for field in required_fields:
            assert field in result, f"Missing required field: {field}"
        
        # Sonnet 4.5 should always be used
        assert result['model_used'] == LLMConfig.PREMIUM_MODEL
    
    @pytest.mark.asyncio
    async def test_automatic_escalation_to_sonnet(self):
        """Test low-confidence Haiku responses escalate to Sonnet"""
        agent = ResumeAnalysisAgent()
        
        # Use ambiguous resume that should trigger escalation
        result = await agent.analyze_candidate(
            resume_text=AMBIGUOUS_RESUME,
            job_requirements=COMPLEX_JOB_REQUIREMENTS
        )
        
        # Verify escalation occurred
        assert result['escalated'] is True
        assert result['model_used'] == LLMConfig.PREMIUM_MODEL
        
        # Sonnet should provide higher confidence
        assert result['confidence'] >= LLMConfig.LOW_CONFIDENCE_THRESHOLD
    
    @pytest.mark.asyncio
    async def test_cost_tracking_accuracy(self):
        """Verify token usage and cost tracking is accurate"""
        agent = ResumeAnalysisAgent()
        
        initial_cost = agent.total_cost
        
        result = await agent.analyze_candidate(
            resume_text=SAMPLE_RESUME,
            job_requirements=SAMPLE_JOB_REQUIREMENTS
        )
        
        # Check token usage was recorded
        assert 'token_usage' in result
        assert result['token_usage']['input'] > 0
        assert result['token_usage']['output'] > 0
        
        # Check cost increased
        assert agent.total_cost > initial_cost
        
        # Verify cost calculation
        expected_cost = LLMConfig.calculate_cost(
            result['token_usage']['input'],
            result['token_usage']['output'],
            result['model_used']
        )
        
        assert abs(agent.total_cost - initial_cost - expected_cost) < 0.01
    
    @pytest.mark.asyncio
    async def test_parallel_agent_execution(self):
        """Test multiple agents can run concurrently"""
        linkedin_agent = LinkedInSourcingAgent()
        github_agent = GitHubSourcingAgent()
        resume_agent = ResumeAnalysisAgent()
        
        start_time = time.time()
        
        # Execute agents in parallel
        results = await asyncio.gather(
            linkedin_agent.analyze(SAMPLE_LINKEDIN_URL),
            github_agent.analyze(SAMPLE_GITHUB_URL),
            resume_agent.analyze_candidate(SAMPLE_RESUME, SAMPLE_JOB_REQUIREMENTS)
        )
        
        duration = time.time() - start_time
        
        # Parallel execution should be faster than sequential
        # 3 agents Ã— 2s each = 6s sequential
        # Parallel should be ~2-3s
        assert duration < 4.0, f"Parallel execution took {duration}s (expected <4s)"
        
        # All agents should return results
        assert len(results) == 3
        assert all(r['confidence'] > 0 for r in results)
```

### Integration Tests

```python
# backend/tests/test_swarm_integration.py

import pytest
from app.agents.enhanced_orchestrator import EnhancedAgentOrchestrator

class TestSwarmIntelligence:
    """Test multi-agent collaboration with Claude 4.5"""
    
    @pytest.mark.asyncio
    async def test_full_candidate_evaluation(self):
        """Test complete candidate evaluation workflow"""
        orchestrator = EnhancedAgentOrchestrator()
        
        result = await orchestrator.evaluate_candidate(
            candidate_id=1,
            job_opening_id=1
        )
        
        # Verify all agents participated
        assert len(result['agent_results']) >= 5
        
        # Verify bias detection ran (always Sonnet 4.5)
        bias_result = next(
            r for r in result['agent_results'] 
            if r['agent_name'] == 'bias_detection'
        )
        assert bias_result['model_used'] == LLMConfig.PREMIUM_MODEL
        
        # Verify consensus was reached
        assert 'consensus' in result
        assert result['consensus']['confidence'] > 0
    
    @pytest.mark.asyncio
    async def test_consensus_with_agent_disagreement(self):
        """Test consensus engine with conflicting agent opinions"""
        orchestrator = EnhancedAgentOrchestrator()
        
        # Create scenario where agents disagree
        # (Mock agent results with conflicting scores)
        agent_results = [
            {'agent': 'linkedin', 'score': 85, 'confidence': 0.9},
            {'agent': 'github', 'score': 45, 'confidence': 0.8},
            {'agent': 'resume', 'score': 70, 'confidence': 0.7},
        ]
        
        consensus = await orchestrator._build_consensus(agent_results)
        
        # Complex consensus should use Sonnet 4.5
        assert consensus['model_used'] == LLMConfig.PREMIUM_MODEL
        
        # Should identify disagreement
        assert consensus['agent_agreement'] == 'disagreement'
        
        # Should provide resolution
        assert 'final_recommendation' in consensus
    
    @pytest.mark.asyncio
    async def test_cost_efficiency_at_scale(self):
        """Test cost remains within budget at scale"""
        orchestrator = EnhancedAgentOrchestrator()
        
        num_candidates = 100
        total_cost = 0
        
        for i in range(num_candidates):
            result = await orchestrator.evaluate_candidate(
                candidate_id=i,
                job_opening_id=1
            )
            total_cost += result['total_cost']
        
        # Average cost should be ~$0.10 per candidate
        avg_cost = total_cost / num_candidates
        
        assert avg_cost < 0.15, f"Average cost ${avg_cost:.2f} exceeds target $0.15"
        
        # Total for 100 candidates should be under $15
        assert total_cost < 15.0, f"Total cost ${total_cost:.2f} exceeds budget $15"
```

### Performance Tests

```python
# backend/tests/test_performance.py

import pytest
import asyncio
from app.agents.resume_agent import ResumeAnalysisAgent

class TestPerformance:
    """Performance benchmarks for Claude 4.5 integration"""
    
    @pytest.mark.asyncio
    async def test_haiku_latency_p95(self):
        """Test Haiku 4.5 meets P95 latency target (<1.5s)"""
        agent = ResumeAnalysisAgent()
        
        latencies = []
        num_requests = 100
        
        for _ in range(num_requests):
            start = time.time()
            await agent.analyze_candidate(SAMPLE_RESUME, SAMPLE_JOB_REQUIREMENTS)
            latencies.append(time.time() - start)
        
        # Calculate P95
        p95_latency = sorted(latencies)[int(num_requests * 0.95)]
        
        assert p95_latency < 1.5, f"P95 latency {p95_latency:.2f}s exceeds 1.5s target"
    
    @pytest.mark.asyncio
    async def test_throughput_capacity(self):
        """Test system can handle target throughput"""
        agent = ResumeAnalysisAgent()
        
        num_concurrent = 50
        start_time = time.time()
        
        # Simulate 50 concurrent requests
        tasks = [
            agent.analyze_candidate(SAMPLE_RESUME, SAMPLE_JOB_REQUIREMENTS)
            for _ in range(num_concurrent)
        ]
        
        results = await asyncio.gather(*tasks)
        duration = time.time() - start_time
        
        # Should complete 50 requests in under 10 seconds
        assert duration < 10.0, f"50 requests took {duration:.2f}s (expected <10s)"
        
        # All requests should succeed
        assert len(results) == num_concurrent
        assert all(r['confidence'] > 0 for r in results)
```

---

## Appendix: Quick Reference

### Model Comparison Table

| Feature | Haiku 4.5 | Sonnet 4.5 |
|---------|-----------|------------|
| **Speed** | âš¡âš¡âš¡ Very Fast | âš¡âš¡ Fast |
| **Intelligence** | ğŸ§ ğŸ§  High | ğŸ§ ğŸ§ ğŸ§  Highest |
| **Cost (Input)** | $1/1M tokens | $3/1M tokens |
| **Cost (Output)** | $5/1M tokens | $15/1M tokens |
| **Context Window** | 200K tokens | 200K tokens |
| **Best For** | High-volume operations | Complex reasoning |
| **Response Time** | 0.8-1.5s (P95) | 1.2-2.5s (P95) |

### Agent Quick Reference

```
HAIKU 4.5 AGENTS (Fast Operations)
â”œâ”€ LinkedIn Sourcing Agent
â”œâ”€ GitHub Sourcing Agent  
â”œâ”€ Resume Analysis Agent
â”œâ”€ Interview Orchestration Agent
â””â”€ Swarm Orchestrator

SONNET 4.5 AGENTS (Complex Reasoning)
â”œâ”€ Bias Detection Agent
â”œâ”€ Predictive Analytics Agent
â””â”€ Consensus Engine
```

### Cost Quick Reference

```
Per Customer/Month (100 candidates):
â”œâ”€ Haiku 4.5 costs:    $4.00
â”œâ”€ Sonnet 4.5 costs:   $5.89
â””â”€ TOTAL:              $9.89

Gross Margin (at $400/recruiter pricing): 97.5% âœ…
```

### API Endpoint Reference

```python
# Health check
GET /api/v1/health

# Agent status
GET /api/v1/agents/status

# Swarm metrics
GET /api/v1/swarm/metrics

# Candidate evaluation
POST /api/v1/evaluate
{
  "candidate_id": 123,
  "job_opening_id": 456
}

# Chat interface
POST /api/v1/chat
{
  "message": "Evaluate senior engineer candidate",
  "conversation_id": "uuid"
}
```

---

## Document Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | Oct 22, 2025 | Initial architecture documentation | Product Team |

---

## Related Documentation

- [Hirely Product Requirements Document v2.0](./Hirely_Product_Requirements_Document_v2_0.md)
- [HoneyBee â†’ Mosaic Strategic Vision](./HoneyBee_to_Mosaic_Strategic_Vision.md)
- [Technical Architecture](./TECHNICAL_ARCHITECTURE.md)

---

**End of Document**
